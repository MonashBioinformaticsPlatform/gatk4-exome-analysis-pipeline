include required(classpath("application"))

backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"

      config {
        # concurrent-job-limit = 100
        # exit-code-timeout-seconds = 300

        filesystems {
          local { }
          http { enabled = true }
        }

        runtime-attributes = """
        Int runtime_minutes = 1440
        Int? cpu
        Float? memory_gb = 8

        String? slurm_account
        String? tmp
        String? singularity_cache
        # Additional path to expose inside the Singularity container(s) (eg /scratch)
        String? singularity_bind_path
        String? docker
        """

        submit = """
            sbatch \
              ${"--account=" + slurm_account} \
              --parsable \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${runtime_minutes} \
              ${default="--cpus-per-task=1" "--cpus-per-task=" + cpu} \
              ${default="--mem=8G" "--mem=" + round(memory_gb) + "G"} \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Ensure singularity is loaded if it's installed as a module
            module load singularity/3.0.2
            ${"export TMPDIR=" + tmp}
            ${"export SINGULARITY_CACHEDIR=" + singularity_cache}
            ${"export SINGULARITY_TMPDIR=" + tmp}
            ${"export SINGULARITY_LOCALCACHEDIR=" + tmp}

            # Build the Docker image into a singularity image
            export IMAGE=$SINGULARITY_CACHEDIR/${docker}.sif

            # Random sleep
            sleep $(printf '%.5f\n' "$(printf '0x0.%04xp1' $RANDOM)")

            # NOTE - by checking if sif file exists, touching and force building,
            #        we prevent multiple concurrent tasks all building the same
            #        singularity image simultaneously
            if [[ ! -f $IMAGE ]]; then
              mkdir -p $(dirname $IMAGE)
              touch $IMAGE
              nice singularity build --force $IMAGE docker://${docker}
            fi

            # Submit the script to SLURM
            sbatch \
              ${"--account=" + slurm_account} \
              --parsable \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${runtime_minutes} \
              ${default="--cpus-per-task=1" "--cpus-per-task=" + cpu} \
              ${default="--mem=8G" "--mem=" + round(memory_gb) + "G"} \
              --wrap "module load singularity/3.0.2; singularity exec ${'--bind ' + singularity_bind_path} --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}"
        """

        kill = "scancel ${job_id}"
        check-alive = "scontrol show job ${job_id}"
        job-id-regex = "(\\d+)"
      }
    }
  }
}
